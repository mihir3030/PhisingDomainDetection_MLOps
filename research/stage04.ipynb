{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    root_dir: Path\n",
    "    load_model_path: Path\n",
    "    load_test_data: Path\n",
    "    local_file: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhishingDomainDetection.constants import CONFIG_FILE_PATH, PARAMS_FILE_APTH\n",
    "from PhishingDomainDetection.utils import read_yaml_file, create_directories, save_reports\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_file_path=CONFIG_FILE_PATH, params_file_path=PARAMS_FILE_APTH):\n",
    "        self.config = read_yaml_file(config_file_path)\n",
    "        self.params = read_yaml_file(params_file_path)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_for_evaluation(self):\n",
    "        self.data_load_config = self.config.training\n",
    "        self.root_dir = self.data_load_config.root_dir\n",
    "        self.file_dir = self.data_load_config.model_save\n",
    "        self.file_path = os.path.join(self.root_dir, self.file_dir)\n",
    "        return self.file_path\n",
    "\n",
    "    def get_test_data_for_evaluation(self):\n",
    "        self.data_load_config = self.config.training\n",
    "        self.root_dir = self.data_load_config.root_dir\n",
    "        self.file_dir = self.data_load_config.test_data_save\n",
    "        self.file_path = os.path.join(self.root_dir, self.file_dir)        \n",
    "        return self.file_path\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        evaluation_config = EvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            load_model_path=Path(self.get_model_for_evaluation()),\n",
    "            load_test_data=Path(self.get_test_data_for_evaluation()),\n",
    "            local_file=Path(config.local_file)\n",
    "        )\n",
    "        \n",
    "        return evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-13 16:27:24,410: INFO common]: yaml file configs\\config.yaml load  successfully\n",
      "[2023-02-13 16:27:24,420: INFO common]: yaml file params.yaml load  successfully\n",
      "[2023-02-13 16:27:24,423: INFO common]: created directory at artifacts\n",
      "[2023-02-13 16:27:24,426: INFO common]: created directory at artifacts/model_score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationConfig(root_dir=WindowsPath('artifacts/model_score'), load_model_path=WindowsPath('artifacts/training/model.pkl'), load_test_data=WindowsPath('artifacts/training/test_data.csv'), local_file=WindowsPath('score.json'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = ConfigurationManager()\n",
    "aa.get_evaluation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_test_data(self):\n",
    "        datapath = self.config.load_test_data\n",
    "        self.df = pd.read_csv(datapath)\n",
    "    \n",
    "    def load_model(self):\n",
    "        model_path = self.config.load_model_path\n",
    "        self.model = joblib.load(model_path)\n",
    "\n",
    "    def evaluation(self):\n",
    "        x_test = self.df.drop(\"phishing\", axis=1)\n",
    "        self.y_test = self.df['phishing']\n",
    "\n",
    "        self.y_pred = self.model.predict(x_test)\n",
    "\n",
    "    \n",
    "    def save_score(self):\n",
    "        test_accuracy = accuracy_score(self.y_test, self.y_pred)\n",
    "        f1_model_score = f1_score(self.y_test, self.y_pred)\n",
    "        scores = {\n",
    "            'test_aaccuracy': test_accuracy,\n",
    "            'f1_score': f1_model_score\n",
    "        }\n",
    "        \n",
    "        evaluation_root_dir = self.config.root_dir\n",
    "        score_file_dir = self.config.local_file\n",
    "        raw_score_file_path = os.path.join(evaluation_root_dir, score_file_dir)\n",
    "\n",
    "        save_reports(scores, raw_score_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-13 16:28:06,034: INFO common]: yaml file configs\\config.yaml load  successfully\n",
      "[2023-02-13 16:28:06,041: INFO common]: yaml file params.yaml load  successfully\n",
      "[2023-02-13 16:28:06,043: INFO common]: created directory at artifacts\n",
      "[2023-02-13 16:28:06,047: INFO common]: created directory at artifacts/model_score\n",
      "report are saved in artifacts\\model_score\\score.json\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "evaluation_config = config.get_evaluation_config()\n",
    "evaluation = Evaluation(config=evaluation_config)\n",
    "evaluation.load_test_data()\n",
    "evaluation.load_model()\n",
    "evaluation.evaluation()\n",
    "evaluation.save_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22ff71f9cfc326748efd7a9104604504f4396b9e94072460122b9aa830a0da70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
